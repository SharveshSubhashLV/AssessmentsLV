# -*- coding: utf-8 -*-
"""Clustering_LAB3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rhLebtocBHkZJirsHjQ-D9jK0vfH-YUv
"""

import pandas as pd
import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
from sklearn.metrics import silhouette_score

df = pd.read_csv("/content/Credit Card Customer Data.csv")

df.head()

df.info()

"""#Null value Handling"""

df.isna().sum()

df['Total_visits_online']=df['Total_visits_online'].fillna(df['Total_visits_online'].mean())

df.isna().sum()

"""#Handling Duplicate values"""

duplicates = df.duplicated(keep=False)
df['dup_bool'] = duplicates

print(df[df['dup_bool'] == True].count())

"""No Duplicates"""

df.drop('dup_bool',axis=1,inplace=True)
df.head(1)

"""#Removing Outliers

using boxplots to check if there are outliers
"""

import seaborn as sns
for column in df.select_dtypes(["int","float"]):
    plt.figure(figsize=(10, 6))  # Set the figure size for better readability
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

"""Removing Outliers"""

for col in df.select_dtypes(["int","float"]):
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower = Q1 - 1.5*IQR
  upper = Q3 + 1.5*IQR
  df = df[df[col]<=upper]
  df = df[df[col]>=lower]

"""Verifying"""

import seaborn as sns
for column in df.select_dtypes(["int","float"]):
    plt.figure(figsize=(10, 6))  # Set the figure size for better readability
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

"""Clearly, there are no outliers for most column and very few for one column, but that will not be a problem

#Performing elbow method before scaling the features for finding the optimal number of clusters
"""

sse = []
k_rng = range(1,10)
for k in k_rng:
   km = KMeans(n_clusters=k)
   km.fit(df)
   sse.append(km.inertia_)

plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse)

"""K can be either 3 or 5, we will try both and find the score and choose accordingly."""

#K=3, as per the inference that we got from elbow method
x = df.select_dtypes(include=['float64', 'int64'])
x

x.drop(['Sl_No','Customer Key'],axis=1,inplace=True)

x

#NO ENCODING NEEDED AS THERE ARE NO CATEGORICAL COLUMNS

kmeans = KMeans(n_clusters=3, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(x)

silhouette_score(x,y_predict)

"""Tyring with 5"""

kmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(x)

silhouette_score(x,y_predict)

"""Therefore, we will go with k=3, as it has a better score

#Performing scaling before building model
"""

from sklearn.preprocessing import MinMaxScaler
m = MinMaxScaler()
x = m.fit_transform(x)

x

"""#Model building"""

kmeans = KMeans(n_clusters=3, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(x)

silhouette_score(x,y_predict)

"""Here, the score was better when it was not scaled, so we will follow that way that is without scaling, when score was better.

Trying the same, without scaling
"""

kmeans = KMeans(n_clusters=3, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(x)

silhouette_score(x,y_predict)

"""#Cluster Profiling"""

df['Cluster'] = kmeans.labels_

df

"""**Plotting all the combinations of columns for the 3 Clusters**"""

df1 = df[df.Cluster==0]
df2 = df[df.Cluster==1]
df3 = df[df.Cluster==2]
"""
original_df1 = df[df.cluster==0]
original_df2 = df[df.cluster==1]
original_df3 = df[df.cluster==2] """

for col1 in df.columns.tolist():
  for col2 in df.columns.tolist():

    plt.scatter(df1[col1],df1[col2],color='green')
    plt.scatter(df2[col1],df2[col2],color='red')
    plt.scatter(df3[col1],df3[col2],color='black')
    plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color='purple',marker='*',label='centroid')
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.legend()
    plt.show()

"""#Cluster Profiling inference"""

df.columns

df1['Total_Credit_Cards'].mean()

df2['Total_Credit_Cards'].mean()

df3['Total_Credit_Cards'].mean()

"""Average Credit Card limit is highest for cluster = 1, we can name that cluster as "High Value Customers"(RED),
For cluster = 0(GREEN coloured), the average credit card limit is the least, they are the "Low Value customers". And for the cluster = 2, the average credit Card limit is greater than the least and lower than the highest, so they are "Moderate Value Customers"(BLACK)

Average Credit Card limit is highest for cluster = 1, we can name that cluster as "High Value Customers",
For cluster = 0(green coloured), the average credit card limit is the least, they are the "Low Value customers". And for the cluster = 2, the average credit Card limit is greater than the least and lower than the highest, so they are "Moderate Value Customers"

Cluster-0 --> LOW VALUE CUSTOMERS
Cluster-1 --> HIGH VALUE CUSTOMERS
Cluster-2 --> MODERATE VALUE CUSTOMERS

#Business Recommendation for different Clusters

HIGH VALUE CUSTOMERS can be given more offers, lesser calls, more number of cards and more online services can ensure these
customers are retained and more such customers are attracted by the bank. They also have higher average bank visits

LOW VALUE CUSTOMERS interestingly have the highest total calls made and have the highest number of online visits.
So, they are more engaged with the bank than the other segments, so creating efficient pipeline of
services that suffice their specific needs will ensure retention of those customers, and also attract more
such customer segments as, in this segment, the volume of segment will impact in our profits.
This segment has the least average number of credit cards.

Moderate value customers have good number of customers but have moderate engagement in all different attributes.
"""